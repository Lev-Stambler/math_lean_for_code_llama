{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# You can initialize the tokenizer here. For example, for BERT:\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "\n",
    "def get_files_in_dir(path, suffix='.txt'):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for next token prediction from files with a specific suffix in the given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The directory path to look for files.\n",
    "    - tokenizer: Tokenizer from the transformers library.\n",
    "    - suffix (str): The file suffix to filter by. Default is '.txt'.\n",
    "    - MAX_CONTENT_LENGTH (int): The maximum length for the input content.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with 'input' and 'target' columns for next token prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists for storing input sequences and target tokens\n",
    "    \n",
    "\n",
    "    # Walking through the directory and its subdirectories\n",
    "    rets = []\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(suffix):\n",
    "                rets.append(os.path.join(dirpath, filename))\n",
    "    return rets\n",
    "\n",
    "def prepare_file_training(file_path: str, tokenizer: CodeLlamaTokenizer, MAX_CONTENT_LENGTH=512, MIN_CONTENT_LENGTH=128):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        # Tokenize the content using the provided tokenizer\n",
    "        tokens = tokenizer.tokenize(content)\n",
    "        # Start at min content length\n",
    "        # for i in range(0, len(tokens) - MIN_CONTENT_LENGTH):  # -1 to leave space for the target token\n",
    "            # print(i, len(tokens))\n",
    "        for length in range(MIN_CONTENT_LENGTH, min(MAX_CONTENT_LENGTH, len(tokens)) + 1):\n",
    "            i = 0\n",
    "            # print(i, length, len(tokens))\n",
    "            # print(i, length, len(tokens))\n",
    "            input_sequence = tokens[i:min(i + length, len(tokens))]\n",
    "            if i + length < len(tokens):\n",
    "                target_token = tokens[i + length]\n",
    "                inputs.append(tokenizer.convert_tokens_to_string(input_sequence))\n",
    "                targets.append(target_token)\n",
    "                \n",
    "        # TODO: clean up\n",
    "        for i in range(0, max(0, len(tokens) - MAX_CONTENT_LENGTH)):\n",
    "            length = MAX_CONTENT_LENGTH\n",
    "            input_sequence = tokens[i:min(i + length, len(tokens))]\n",
    "            if i + length < len(tokens):\n",
    "                target_token = tokens[i + length]\n",
    "                inputs.append(tokenizer.convert_tokens_to_string(input_sequence))\n",
    "                targets.append(target_token)\n",
    "\n",
    "    return pd.DataFrame({'input': inputs, 'target': targets})\n",
    "\n",
    "\n",
    "path = '../../../lean/mathlib4/Mathlib/'\n",
    "\n",
    "MAX_CONTENT_LENGTH=2_048\n",
    "lean_files = get_files_in_dir(path, \".lean\")\n",
    "df = prepare_file_training(lean_files[0], tokenizer, MAX_CONTENT_LENGTH, MIN_CONTENT_LENGTH=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(lean_files)\n",
    "n_train_files = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "test_file_paths = np.random.choice(np.arange(n_files), n_train_files).tolist()\n",
    "def save_prepared_file(save_label: str, file_path: str, tokenizer: CodeLlamaTokenizer, MAX_CONTENT_LENGTH=512, MIN_CONTENT_LENGTH=128):\n",
    "\t\tdf = prepare_file_training(file_path, tokenizer, MAX_CONTENT_LENGTH, MIN_CONTENT_LENGTH)\n",
    "\t\tsave_path = os.path.join('data/train', save_label)\n",
    "\t\tdf.to_json(save_path + '.json', index=False)\n",
    "\n",
    "for file_idx in test_file_paths:\n",
    "\tfile_path = lean_files[file_idx]\n",
    "\tsave_prepared_file('testing_' + str(file_idx), file_path, tokenizer, MAX_CONTENT_LENGTH, MIN_CONTENT_LENGTH=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actic\n",
      "import Mathlib.Tactic.Abel\n",
      "import Mathlib.Tactic.ApplyCongr\n",
      "import Mathlib.Tactic.ApplyFun\n",
      "import Mathlib.Tactic.ApplyWith\n",
      "import Mathlib.Tactic.Attr.Core\n",
      "import Mathlib.Tactic.Attr.Register\n",
      "import Mathlib.Tactic.Backtrack\n",
      "import Mathlib.Tactic.Basic\n",
      "import Mathlib.Tactic.ByContra\n",
      "import Mathlib.Tactic.Cache\n",
      "import Mathlib.Tactic.CancelDenoms\n",
      "import Mathlib.Tactic.CancelDenoms.Core\n",
      "import Mathlib.Tactic\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.target[7]), print(df.input[8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
