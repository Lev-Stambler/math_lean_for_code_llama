{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-6m11epe0\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-6m11epe0\n",
      "  Resolved https://github.com/huggingface/transformers to commit 9ed538f2e67ee10323d96c97284cf83d44f0c507\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.34.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/lev/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lev/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lev/.local/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.34.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lev/.local/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.34.0.dev0) (2020.6.20)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers -U\n",
    "!pip install -q huggingface_hub\n",
    "!pip install -q -U trl transformers accelerate peft\n",
    "!pip install -q -U datasets bitsandbytes einops wandb\n",
    "!pip install -q -U sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'mathlib4' already exists and is not an empty directory.\n",
      "already cloned\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/leanprover-community/mathlib4.git || echo \"already cloned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_name = \"codellama/CodeLlama-7b-hf\"\n",
    "# You can initialize the tokenizer here. For example, for BERT:\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "def get_files_in_dir(path, suffix='.txt'):\n",
    "    \"\"\"\n",
    "    Prepares a dataset for next token prediction from files with a specific suffix in the given directory and its subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    - path (str): The directory path to look for files.\n",
    "    - tokenizer: Tokenizer from the transformers library.\n",
    "    - suffix (str): The file suffix to filter by. Default is '.txt'.\n",
    "    - MAX_CONTENT_LENGTH (int): The maximum length for the input content.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with 'input' and 'target' columns for next token prediction.\n",
    "    \"\"\"\n",
    "\n",
    "    # Lists for storing input sequences and target tokens\n",
    "    \n",
    "\n",
    "    # Walking through the directory and its subdirectories\n",
    "    rets = []\n",
    "    for dirpath, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(suffix):\n",
    "                rets.append(os.path.join(dirpath, filename))\n",
    "    return rets\n",
    "\n",
    "def prepare_file_examples(file_path: str, tokenizer: CodeLlamaTokenizer, MAX_CONTENT_LENGTH=512, MIN_CONTENT_LENGTH=128):\n",
    "    outs = []\n",
    "    # inputs = []\n",
    "    # targets = []\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "        content = file.read()\n",
    "        # Tokenize the content using the provided tokenizer\n",
    "        tokens = tokenizer.tokenize(content)\n",
    "        # Start at min content length\n",
    "        # for i in range(0, len(tokens) - MIN_CONTENT_LENGTH):  # -1 to leave space for the target token\n",
    "            # print(i, len(tokens))\n",
    "        for length in range(MIN_CONTENT_LENGTH, min(MAX_CONTENT_LENGTH, len(tokens)) + 1):\n",
    "            i = 0\n",
    "            # print(i, length, len(tokens))\n",
    "            # print(i, length, len(tokens))\n",
    "            input_sequence = tokens[i:min(i + length, len(tokens))]\n",
    "            if i + length < len(tokens):\n",
    "                target_token = tokens[i + length]\n",
    "                outs.append({'text': tokenizer.convert_tokens_to_string(input_sequence), 'label': target_token})\n",
    "                \n",
    "        # TODO: clean up\n",
    "        for i in range(0, max(0, len(tokens) - MAX_CONTENT_LENGTH)):\n",
    "            length = MAX_CONTENT_LENGTH\n",
    "            input_sequence = tokens[i:min(i + length, len(tokens))]\n",
    "            if i + length < len(tokens):\n",
    "                target_token = tokens[i + length]\n",
    "                outs.append({'text': tokenizer.convert_tokens_to_string(input_sequence), 'label': target_token})\n",
    "\n",
    "    return outs\n",
    "\n",
    "\n",
    "\n",
    "def prepare_json(files):#, tokenizer, MAX_CONTENT_LENGTH):\n",
    "\t# r = []\n",
    "    d = {\"train\": []}\n",
    "\t# with jsonlines.open('data_simple/test.jsonl', mode='w') as writer:\n",
    "    for i, file in enumerate(files):\n",
    "        # with open(file, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "    # content = file.read()\n",
    "        d['train'] += prepare_file_examples(file, tokenizer, MAX_CONTENT_LENGTH, MIN_CONTENT_LENGTH=128)\n",
    "        json.dump(d['train'], open('data_simple/test.json', 'w'))\n",
    "        print(\"Finished processing file\", i + 1, \"of\", len(files), \"files\")\n",
    "        # r.append(prepare_file_training(file))#, tokenizer, MAX_CONTENT_LENGTH)\n",
    "    # json.dump(d['train'], open('data_simple/test.json', 'w'))\n",
    "#  Peep to clean https://huggingface.co/docs/datasets/index\n",
    "\n",
    "\n",
    "\n",
    "path = '../../../lean/mathlib4/Mathlib/'\n",
    "# path = './mathlib4'\n",
    "\n",
    "MAX_CONTENT_LENGTH=2_048\n",
    "lean_files = get_files_in_dir(path, \".lean\")\n",
    "# df = prepare_file_training(lean_files[0], tokenizer, MAX_CONTENT_LENGTH, MIN_CONTENT_LENGTH=1)\n",
    "# prepare_jsonl(lean_files)\n",
    "# https://medium.com/@ud.chandra/instruction-fine-tuning-llama-2-with-pefts-qlora-method-d6a801ebb19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘data_simple’: File exists\n",
      "data_simple dir already exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data_simple || echo \"data_simple dir already exists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing file 1 of 10 files\n",
      "Finished processing file 2 of 10 files\n",
      "Finished processing file 3 of 10 files\n",
      "Finished processing file 4 of 10 files\n",
      "Finished processing file 5 of 10 files\n",
      "Finished processing file 6 of 10 files\n",
      "Finished processing file 7 of 10 files\n",
      "Finished processing file 8 of 10 files\n",
      "Finished processing file 9 of 10 files\n",
      "Finished processing file 10 of 10 files\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "n_files = len(lean_files)\n",
    "n_train_files = 10\n",
    "test_file_paths = np.random.choice(lean_files, n_train_files).tolist()\n",
    "prepare_json(test_file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLora this boy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b6a7557c5694cbcb2165108a0ea1c3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bb14b3e1d14d9e96b36b1226f7909a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd6b2443fc64ecbb213fd9b516663fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 53315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={'train': [\"data_simple/test.json\"]})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 14:03:48.890901: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-02 14:03:48.932215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 14:03:49.372425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c1fb4770d145789723df9dc0efa75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# When prompted, paste the HF access token you created earlier.\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer, TrainingArguments\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# dataset_name = \"<your_hf_dataset>\"\n",
    "# dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    # Added by Lev, remove\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "device_map = {\"\": 0}\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "\n",
    "    # Added by Lev, remove\n",
    "    load_in_4bit=True,\n",
    "    # use_auth_token=True\n",
    ")\n",
    "# model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\",\n",
    "#     device_map='auto',\n",
    "#     # load_in_4bit=True,\n",
    "#     device_map=device_map,\n",
    "#     trust_remote_code=True,\n",
    "#     quantization_config=bnb_config\n",
    "# )\n",
    "base_model = model\n",
    "base_model.config.use_cache = False\n",
    "\n",
    "# More info: https://github.com/huggingface/transformers/pull/24906\n",
    "base_model.config.pretraining_tp = 1 \n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "output_dir = \"./results\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    max_steps=500\n",
    ")\n",
    "\n",
    "# max_seq_length = 1_024 # TODO: TUNE!\n",
    "max_seq_length = 512 # TODO: TUNE!\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "import os\n",
    "output_dir = os.path.join(output_dir, \"final_checkpoint\")\n",
    "trainer.model.save_pretrained(output_dir)\n",
    "# TODO: now we are just out of memory..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
