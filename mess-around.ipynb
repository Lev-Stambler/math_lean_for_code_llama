{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/huggingface/transformers\n",
      "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-89h6sde1\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-89h6sde1\n",
      "  Resolved https://github.com/huggingface/transformers to commit 6ae71ec8369f71490311b30c902ea6990efda4f0\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.16.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.34.0.dev0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lev/.local/lib/python3.9/site-packages (from transformers==4.34.0.dev0) (4.66.1)\n",
      "Requirement already satisfied: fsspec in /home/lev/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lev/.local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0.dev0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lev/.local/lib/python3.9/site-packages (from requests->transformers==4.34.0.dev0) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers==4.34.0.dev0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers==4.34.0.dev0) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers==4.34.0.dev0) (2020.6.20)\n",
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# !pip install transformers -U\n",
    "!pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU bitsandbytes transformers datasets accelerate loralib einops xformers\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3e1b6d4bec4b42a2e30852c367d10f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b1ced5c9ad4e39bcc7974dde827395",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c5bab9eb1045a7a6467fdffee90803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c7119eb111643ddb951c169d8dc49ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeb66c73a7545aaa3e8f995ce7e831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7de194b14242188741d5ed35068c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, CodeLlamaTokenizer, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = CodeLlamaTokenizer.from_pretrained(\"codellama/CodeLlama-7b-hf\")\n",
    "model = LlamaForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-hf\",\n",
    "    device_map='auto',\n",
    "    load_in_4bit=True,\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type='nf4'\n",
    "\t\t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bitsandbytes as bnb\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from datasets import load_dataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "# https://github.com/tloen/alpaca-lora for q_proj v_proj\n",
    "\n",
    "# https://github.com/mzbac/qlora-fine-tune TODO: PEEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training Data\n",
    "<!-- https://medium.com/@amodwrites/a-definitive-guide-to-qlora-fine-tuning-falcon-7b-with-peft-78f500a1f337 -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbe853a5b7c34953ac444675cdd1e4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3512 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "  return f\"\"\"\n",
    "<Human>: {data_point[\"Context\"]}\n",
    "<AI>: {data_point[\"Response\"]}\n",
    "  \"\"\".strip()\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "  full_prompt = generate_prompt(data_point)\n",
    "  tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
    "  return tokenized_full_prompt\n",
    "\n",
    "dataset_name = 'Amod/mental_health_counseling_conversations'\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "dataset = dataset.shuffle().map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Context': \"I'm a female in my mid 20s.  Lately I tend to over drink and I've become a very angry drunk. \\r\\n\\r\\nIn the past, I have even cheated on my boyfriend while I was under the influence of alcohol. \\r\\n\\r\\nBut now, even if I don't do anything wrong and don't embarrass myself, I still feel really guilty after a night of drinking. I don't understand why I'm feeling this way. Does this mean I have a problem?\",\n",
       " 'Response': 'Hello, I commend you for your courage in taking a look at the role alcohol has in your life. It sounds like you\\'re concerned about what happens when you drink too much and I suspect you already know the answer to your question about whether you have a problem or not. I imagine you would like to stop feeling guilty and would like to avoid cheating on your boyfriend or other negative consequences and maybe have a fear of being or becoming an \"addict\" or \"alcoholic.\" You might have a \"problem\" but that does not necessarily mean that you are an addict.\\xa0I don\\'t have information to know if \"addiction\" or \"dependence\" or other words would best describe where you are with drinking, but it sounds like it\\'s begun to have some negatives, so forgive me using words like addict, dependence and so on. I mean it more as a road map than a diagnosis. The feedback I\\'m writing here is very general and doesn\\'t address physical dependence and many other factors that might apply to your situation. \\xa0One of the ways to think about substance (mis)use is to think of addiction as a disease of avoidance. Let me repeat that: it is a disease of avoidance. Your ultimate task in living a balanced life is to figure out what you\\'re avoiding and develop other ways to manage those feelings, experiences, and so on. And of course, along the way, you may want to look at triggers, situations, biological vulnerability, social pressures, coping skills, relapse prevention planning and so on. Depending on where you are in your drinking, you might very well benefit from expertise and support.\\xa0Remember also that alcohol depresses our central nervous system and disinhibits us. That means that alcohol is often a substance of choice to relax, destress, calm down, etc. Also, it allows feelings, thoughts, and behaviors that we usually inhibit to be expressed. If you were unfaithful and often angry, that\\'s your first signpost. For angry drinkers, it is often true that you don\\'t drink and then get angry, you drink in order to express anger.\\xa0I recommend you find someone you can speak frankly with, who is knowledgeable about addiction. Wishing you the best health and wellness.\\xa0',\n",
       " 'input_ids': [1,\n",
       "  529,\n",
       "  29950,\n",
       "  7889,\n",
       "  23917,\n",
       "  306,\n",
       "  29915,\n",
       "  29885,\n",
       "  263,\n",
       "  12944,\n",
       "  297,\n",
       "  590,\n",
       "  7145,\n",
       "  29871,\n",
       "  29906,\n",
       "  29900,\n",
       "  29879,\n",
       "  29889,\n",
       "  29871,\n",
       "  365,\n",
       "  2486,\n",
       "  306,\n",
       "  10331,\n",
       "  304,\n",
       "  975,\n",
       "  13748,\n",
       "  322,\n",
       "  306,\n",
       "  29915,\n",
       "  345,\n",
       "  4953,\n",
       "  263,\n",
       "  1407,\n",
       "  26230,\n",
       "  4192,\n",
       "  2960,\n",
       "  29889,\n",
       "  6756,\n",
       "  13,\n",
       "  30004,\n",
       "  13,\n",
       "  797,\n",
       "  278,\n",
       "  4940,\n",
       "  29892,\n",
       "  306,\n",
       "  505,\n",
       "  1584,\n",
       "  923,\n",
       "  630,\n",
       "  373,\n",
       "  590,\n",
       "  8023,\n",
       "  18326,\n",
       "  1550,\n",
       "  306,\n",
       "  471,\n",
       "  1090,\n",
       "  278,\n",
       "  9949,\n",
       "  310,\n",
       "  27231,\n",
       "  5391,\n",
       "  29889,\n",
       "  6756,\n",
       "  13,\n",
       "  30004,\n",
       "  13,\n",
       "  6246,\n",
       "  1286,\n",
       "  29892,\n",
       "  1584,\n",
       "  565,\n",
       "  306,\n",
       "  1016,\n",
       "  29915,\n",
       "  29873,\n",
       "  437,\n",
       "  3099,\n",
       "  2743,\n",
       "  322,\n",
       "  1016,\n",
       "  29915,\n",
       "  29873,\n",
       "  21620,\n",
       "  26771,\n",
       "  6142,\n",
       "  29892,\n",
       "  306,\n",
       "  1603,\n",
       "  4459,\n",
       "  2289,\n",
       "  27719,\n",
       "  1156,\n",
       "  263,\n",
       "  4646,\n",
       "  310,\n",
       "  13748,\n",
       "  292,\n",
       "  29889,\n",
       "  306,\n",
       "  1016,\n",
       "  29915,\n",
       "  29873,\n",
       "  2274,\n",
       "  2020,\n",
       "  306,\n",
       "  29915,\n",
       "  29885,\n",
       "  11223,\n",
       "  445,\n",
       "  982,\n",
       "  29889,\n",
       "  5538,\n",
       "  445,\n",
       "  2099,\n",
       "  306,\n",
       "  505,\n",
       "  263,\n",
       "  1108,\n",
       "  29973,\n",
       "  13,\n",
       "  29966,\n",
       "  23869,\n",
       "  23917,\n",
       "  15043,\n",
       "  29892,\n",
       "  306,\n",
       "  844,\n",
       "  355,\n",
       "  366,\n",
       "  363,\n",
       "  596,\n",
       "  19872,\n",
       "  297,\n",
       "  5622,\n",
       "  263,\n",
       "  1106,\n",
       "  472,\n",
       "  278,\n",
       "  6297,\n",
       "  27231,\n",
       "  5391,\n",
       "  756,\n",
       "  297,\n",
       "  596,\n",
       "  2834,\n",
       "  29889,\n",
       "  739,\n",
       "  10083,\n",
       "  763,\n",
       "  366,\n",
       "  29915,\n",
       "  276,\n",
       "  15041,\n",
       "  1048,\n",
       "  825,\n",
       "  5930,\n",
       "  746,\n",
       "  366,\n",
       "  13748,\n",
       "  2086,\n",
       "  1568,\n",
       "  322,\n",
       "  306,\n",
       "  12326,\n",
       "  366,\n",
       "  2307,\n",
       "  1073,\n",
       "  278,\n",
       "  1234,\n",
       "  304,\n",
       "  596,\n",
       "  1139,\n",
       "  1048,\n",
       "  3692,\n",
       "  366,\n",
       "  505,\n",
       "  263,\n",
       "  1108,\n",
       "  470,\n",
       "  451,\n",
       "  29889,\n",
       "  306,\n",
       "  14034,\n",
       "  366,\n",
       "  723,\n",
       "  763,\n",
       "  304,\n",
       "  5040,\n",
       "  11223,\n",
       "  27719,\n",
       "  322,\n",
       "  723,\n",
       "  763,\n",
       "  304,\n",
       "  4772,\n",
       "  923,\n",
       "  1218,\n",
       "  373,\n",
       "  596,\n",
       "  8023,\n",
       "  18326,\n",
       "  470,\n",
       "  916,\n",
       "  8178,\n",
       "  27721,\n",
       "  322,\n",
       "  5505,\n",
       "  505,\n",
       "  263,\n",
       "  8866,\n",
       "  310,\n",
       "  1641,\n",
       "  470,\n",
       "  14171,\n",
       "  385,\n",
       "  376,\n",
       "  1202,\n",
       "  919,\n",
       "  29908,\n",
       "  470,\n",
       "  376,\n",
       "  284,\n",
       "  1111,\n",
       "  5391,\n",
       "  293,\n",
       "  1213,\n",
       "  887,\n",
       "  1795,\n",
       "  505,\n",
       "  263,\n",
       "  376,\n",
       "  17199,\n",
       "  29908,\n",
       "  541,\n",
       "  393,\n",
       "  947,\n",
       "  451,\n",
       "  12695,\n",
       "  2099,\n",
       "  393,\n",
       "  366,\n",
       "  526,\n",
       "  385,\n",
       "  788,\n",
       "  919,\n",
       "  29889,\n",
       "  30081,\n",
       "  29902,\n",
       "  1016,\n",
       "  29915,\n",
       "  29873,\n",
       "  505,\n",
       "  2472,\n",
       "  304,\n",
       "  1073,\n",
       "  565,\n",
       "  376,\n",
       "  1202,\n",
       "  2463,\n",
       "  29908,\n",
       "  470,\n",
       "  376,\n",
       "  2716,\n",
       "  355,\n",
       "  663,\n",
       "  29908,\n",
       "  470,\n",
       "  916,\n",
       "  3838,\n",
       "  723,\n",
       "  1900,\n",
       "  8453,\n",
       "  988,\n",
       "  366,\n",
       "  526,\n",
       "  411,\n",
       "  13748,\n",
       "  292,\n",
       "  29892,\n",
       "  541,\n",
       "  372,\n",
       "  10083,\n",
       "  763,\n",
       "  372,\n",
       "  29915,\n",
       "  29879,\n",
       "  23580,\n",
       "  304,\n",
       "  505,\n",
       "  777,\n",
       "  3480,\n",
       "  5056,\n",
       "  29892,\n",
       "  577,\n",
       "  18879,\n",
       "  573,\n",
       "  592,\n",
       "  773,\n",
       "  3838,\n",
       "  763,\n",
       "  788,\n",
       "  919,\n",
       "  29892,\n",
       "  26307,\n",
       "  322,\n",
       "  577,\n",
       "  373,\n",
       "  29889,\n",
       "  306,\n",
       "  2099,\n",
       "  372,\n",
       "  901,\n",
       "  408,\n",
       "  263,\n",
       "  6520,\n",
       "  2910,\n",
       "  1135,\n",
       "  263,\n",
       "  24876,\n",
       "  19263,\n",
       "  29889,\n",
       "  450,\n",
       "  16705,\n",
       "  306,\n",
       "  29915,\n",
       "  29885,\n",
       "  5007,\n",
       "  1244,\n",
       "  338,\n",
       "  1407,\n",
       "  2498,\n",
       "  322,\n",
       "  1838,\n",
       "  29915,\n",
       "  29873,\n",
       "  3211,\n",
       "  9128,\n",
       "  26307,\n",
       "  322,\n",
       "  1784,\n",
       "  916,\n",
       "  13879,\n",
       "  393,\n",
       "  1795,\n",
       "  3394,\n",
       "  304,\n",
       "  596,\n",
       "  6434,\n",
       "  29889,\n",
       "  20246,\n",
       "  6716,\n",
       "  310,\n",
       "  278,\n",
       "  5837,\n",
       "  304,\n",
       "  1348,\n",
       "  1048,\n",
       "  5960,\n",
       "  749,\n",
       "  313,\n",
       "  26737,\n",
       "  29897,\n",
       "  1509,\n",
       "  338,\n",
       "  304,\n",
       "  1348,\n",
       "  310,\n",
       "  788,\n",
       "  2463,\n",
       "  408,\n",
       "  263,\n",
       "  17135,\n",
       "  310,\n",
       "  4772,\n",
       "  749,\n",
       "  29889,\n",
       "  2803,\n",
       "  592,\n",
       "  12312,\n",
       "  393,\n",
       "  29901,\n",
       "  372,\n",
       "  338,\n",
       "  263,\n",
       "  17135,\n",
       "  310,\n",
       "  4772,\n",
       "  749,\n",
       "  29889,\n",
       "  3575,\n",
       "  8494,\n",
       "  6490,\n",
       "  3414,\n",
       "  297,\n",
       "  8471,\n",
       "  263,\n",
       "  6411,\n",
       "  8362,\n",
       "  2834,\n",
       "  338,\n",
       "  304,\n",
       "  4377,\n",
       "  714,\n",
       "  825,\n",
       "  366,\n",
       "  29915,\n",
       "  276,\n",
       "  4772,\n",
       "  292,\n",
       "  322,\n",
       "  2693,\n",
       "  916,\n",
       "  5837,\n",
       "  304,\n",
       "  10933,\n",
       "  1906,\n",
       "  21737,\n",
       "  29892,\n",
       "  27482,\n",
       "  29892,\n",
       "  322,\n",
       "  577,\n",
       "  373,\n",
       "  29889,\n",
       "  1126,\n",
       "  310,\n",
       "  3236,\n",
       "  29892,\n",
       "  3412,\n",
       "  278,\n",
       "  982,\n",
       "  29892,\n",
       "  366,\n",
       "  1122,\n",
       "  864,\n",
       "  304,\n",
       "  1106,\n",
       "  472,\n",
       "  23660,\n",
       "  29892,\n",
       "  18845,\n",
       "  29892,\n",
       "  4768,\n",
       "  5996,\n",
       "  23180,\n",
       "  3097,\n",
       "  29892,\n",
       "  5264,\n",
       "  3965,\n",
       "  1973,\n",
       "  29892,\n",
       "  5614,\n",
       "  292,\n",
       "  25078,\n",
       "  29892,\n",
       "  337,\n",
       "  13938,\n",
       "  5557,\n",
       "  291,\n",
       "  18987,\n",
       "  322,\n",
       "  577,\n",
       "  373,\n",
       "  29889,\n",
       "  28277,\n",
       "  373,\n",
       "  988,\n",
       "  366,\n",
       "  526,\n",
       "  297,\n",
       "  596,\n",
       "  13748,\n",
       "  292,\n",
       "  29892,\n",
       "  366,\n",
       "  1795,\n",
       "  1407,\n",
       "  1532,\n",
       "  14169,\n",
       "  515,\n",
       "  17924,\n",
       "  895,\n",
       "  322,\n",
       "  2304,\n",
       "  29889,\n",
       "  30081,\n",
       "  7301,\n",
       "  1096,\n",
       "  884,\n",
       "  393,\n",
       "  27231,\n",
       "  5391,\n",
       "  316,\n",
       "  2139,\n",
       "  267,\n",
       "  1749,\n",
       "  6555,\n",
       "  23547,\n",
       "  681,\n",
       "  1788,\n",
       "  322,\n",
       "  766,\n",
       "  262,\n",
       "  6335,\n",
       "  1169,\n",
       "  502,\n",
       "  29889,\n",
       "  2193,\n",
       "  2794,\n",
       "  393,\n",
       "  27231,\n",
       "  5391,\n",
       "  338,\n",
       "  4049,\n",
       "  263,\n",
       "  5960,\n",
       "  749,\n",
       "  310,\n",
       "  7348,\n",
       "  304,\n",
       "  26681,\n",
       "  29892,\n",
       "  2731,\n",
       "  1253,\n",
       "  29892,\n",
       "  21732,\n",
       "  1623,\n",
       "  29892,\n",
       "  2992,\n",
       "  29889,\n",
       "  3115,\n",
       "  29892,\n",
       "  372,\n",
       "  6511,\n",
       "  21737,\n",
       "  29892,\n",
       "  13133,\n",
       "  29892,\n",
       "  322,\n",
       "  4010,\n",
       "  18930,\n",
       "  393,\n",
       "  591,\n",
       "  5491,\n",
       "  297,\n",
       "  6335,\n",
       "  277,\n",
       "  304,\n",
       "  367,\n",
       "  13384,\n",
       "  29889,\n",
       "  960,\n",
       "  366,\n",
       "  892,\n",
       "  443,\n",
       "  5444,\n",
       "  389,\n",
       "  1319,\n",
       "  322,\n",
       "  4049,\n",
       "  26230,\n",
       "  29892,\n",
       "  393,\n",
       "  29915,\n",
       "  29879,\n",
       "  596,\n",
       "  937,\n",
       "  1804,\n",
       "  2490,\n",
       "  29889,\n",
       "  1152,\n",
       "  26230,\n",
       "  13748,\n",
       "  414,\n",
       "  29892,\n",
       "  372,\n",
       "  338,\n",
       "  4049,\n",
       "  1565,\n",
       "  393,\n",
       "  366,\n",
       "  1016,\n",
       "  29915,\n",
       "  29873,\n",
       "  13748,\n",
       "  322,\n",
       "  769,\n",
       "  679,\n",
       "  26230,\n",
       "  29892,\n",
       "  366,\n",
       "  13748,\n",
       "  297,\n",
       "  1797,\n",
       "  304,\n",
       "  4653,\n",
       "  27343,\n",
       "  29889,\n",
       "  30081,\n",
       "  29902,\n",
       "  6907,\n",
       "  366,\n",
       "  1284,\n",
       "  4856,\n",
       "  366,\n",
       "  508,\n",
       "  7726,\n",
       "  2524,\n",
       "  29895,\n",
       "  368,\n",
       "  411,\n",
       "  29892,\n",
       "  1058,\n",
       "  338,\n",
       "  7134,\n",
       "  519,\n",
       "  1048,\n",
       "  788,\n",
       "  2463,\n",
       "  29889,\n",
       "  399,\n",
       "  14424,\n",
       "  366,\n",
       "  278,\n",
       "  1900,\n",
       "  9045,\n",
       "  322,\n",
       "  1532,\n",
       "  2264,\n",
       "  29889],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: via fast.ai\n",
    "# Nice\n",
    "\"\"\"\n",
    "from fastai.text import *\n",
    "\n",
    "# Path to your data\n",
    "path = Path('data/texts')\n",
    "\n",
    "# Create a DataBunch suitable for language modeling\n",
    "data_lm = (TextList.from_folder(path)\n",
    "           .filter_by_folder(include=['.'])  # Include all files in the root directory of 'path'\n",
    "           .split_by_rand_pct(0.1)  # Randomly split 10% of data for validation\n",
    "           .label_for_lm()  # Use this method for a language model\n",
    "           .databunch(bs=64, num_workers=1))\n",
    "\n",
    "data_lm.save('data_lm.pkl')\n",
    "\"\"\"\n",
    "# From chatgpt\n",
    "dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'out'\n",
    "\n",
    "training_args = transformers.TrainingArguments(\n",
    "    auto_find_batch_size=True,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    save_total_limit=4,\n",
    "    logging_steps=10,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    save_strategy='epoch',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-27 15:09:10.185371: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-27 15:09:10.200525: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-27 15:09:10.645403: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39;49mTrainer(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39;49mdataset,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     args\u001b[39m=\u001b[39;49mtraining_args,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     data_collator\u001b[39m=\u001b[39;49mtransformers\u001b[39m.\u001b[39;49mDataCollatorForLanguageModeling(tokenizer, mlm\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdebian-desktop/home/lev/code/research/ai/code_llama_lean/mess-around.ipynb#X16sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/trainer.py:412\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[39m# At this stage the model is already loaded\u001b[39;00m\n\u001b[1;32m    411\u001b[0m \u001b[39mif\u001b[39;00m _is_quantized_and_base_model \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_peft_model:\n\u001b[0;32m--> 412\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    413\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    414\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m for more details\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[39melif\u001b[39;00m _is_quantized_and_base_model \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(model, \u001b[39m\"\u001b[39m\u001b[39m_is_quantized_training_enabled\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    418\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    419\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe model you want to train is loaded in 8-bit precision.  if you want to fine-tune an 8-bit\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    420\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m model, please make sure that you have installed `bitsandbytes>=0.37.0`. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    421\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ]
    }
   ],
   "source": [
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "model.config.use_cache = False\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
